{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "import time \n",
    "from IPython import  display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, w, b):\n",
    "    return a[0]*np.sin(w[0]*x+b[0])+a[1]*np.sin(2*w[1]*x+b[1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N):\n",
    "    \n",
    "    param_set = {'w':[], 'a':[], 'b':[], 'mu_x':[]}\n",
    "    datasets = []\n",
    "    for i in range(N):\n",
    "        Size = np.random.randint(low=4, high=50)\n",
    "        w = np.random.uniform(low=5, high=7, size=2)\n",
    "        b = np.random.uniform(low=0, high=2*np.pi, size=2)\n",
    "        a = np.random.normal(loc=0, scale=1.0, size=2)\n",
    "        mu_x = np.random.uniform(low=-4, high=4, size=1)\n",
    "\n",
    "        param_set['w'].append(w); param_set['b'].append(b); param_set['a'].append(a); param_set['mu_x'].append(mu_x); \n",
    "        \n",
    "        x = np.random.normal(loc=mu_x, scale=1.0, size=Size)\n",
    "        y = np.random.normal(loc=f(x, a, w, b), scale=1.0, size=Size)\n",
    "    \n",
    "        datasets.append((x, y))\n",
    "        \n",
    "    return datasets, param_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, param_set = generate_dataset(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(datasets, batchsize):\n",
    "    \n",
    "    for d in datasets:\n",
    "        indices = np.random.permutation(np.arange(len(d[0])))\n",
    "        for start in range(0, len(indices), batchsize):\n",
    "            ix = indices[start: start + batchsize]\n",
    "            \n",
    "            yield  torch.FloatTensor(d[0][ix]), torch.FloatTensor(d[1][ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = iterate_minibatches(datasets, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.3031,  1.7793,  4.3289,  2.4372]),\n",
       " tensor([ 0.1408,  0.6248,  0.1443,  0.0805]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_block(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        \n",
    "        super(self.__class__, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(dim_in, dim_out), \n",
    "                                nn.LayerNorm(dim_out),\n",
    "                                nn.ReLU())\n",
    "        def forward(self, X, z):\n",
    "            \n",
    "            C = torch.cat([X, z], dim=-1)\n",
    "            return self.fc(C), X\n",
    "            \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_z, in_x, dim_middle, dim_out, n_layers):\n",
    "        \"\"\" \n",
    "        nn that maps from latent space Z and the sample X to the target variable Y\n",
    "        \n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.fc1 = fc_block(in_z+in_x, dim_middle)\n",
    "                \n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            fc_block(in_x+in_x, dim_middle)\n",
    "            for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_last_mu = nn.Linear(dim_middle, dim_out)\n",
    "        self.fc_last_sigma = nn.Linear(dim_middle, dim_out)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    " \n",
    "    def forward(self, X, z):\n",
    "        \"\"\"\n",
    "        takes x and latent z and produces target y\n",
    "        \"\"\"\n",
    "  \n",
    "        c, c_prev = self.fc1(X, z)\n",
    "\n",
    "        for fc in self.layer_stack:\n",
    "            c, c_prev = l(c, c_prev)\n",
    "\n",
    "        self.mu_y = self.fc_last_mu(c)\n",
    "        self.sigma_y = self.sigmoid(self.fc_last_sigma(c))*0.1+0.001\n",
    "\n",
    "\n",
    "        return self.mu_y, self.sigma_y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N_batch, in_x, dim_middle, dim_out, n_layers):\n",
    "        \"\"\" \n",
    "        nn that maps dataset to the latent z that represent it      \n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        # пока самая простая \n",
    "        self.fc1 = nn.Linear(in_x, dim_middle)\n",
    "        \n",
    "        self.fc_last_mu = nn.Linear(dim_middle, dim_out)\n",
    "        self.fc_last_sigma = nn.Linear(dim_middle, dim_out)\n",
    "        \n",
    " \n",
    "    def forward(self, S):\n",
    "        \"\"\"\n",
    "        takes dataset S and produces mu and sigma of normal distribution z\n",
    "        \"\"\"\n",
    "  \n",
    "        S = self.fc1(S)    \n",
    "        S = S.mean(0)\n",
    "        \n",
    "        self.mu_z = self.fc_last_mu(S)\n",
    "        self.sigma_z = self.fc_last_sigma(S).exp() # экспонента чтобы не было отрицательных значений дисперсии\n",
    "\n",
    "        return self.mu_z, self.sigma_z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ln p(D) \\ge \\sum_j E_{N(z | 0, 1)} \\sum_i \\ln p(y_{ij} | x_{ij}, h_\\alpha(\\mu_z+\\Sigma_z^{1/2} z)) - \\sum_j KL(q_{\\theta_j}(z_j, S_j) || p(z_j)) $$\n",
    "\n",
    "${KL({\\mathcal {N}}_{0}\\|{\\mathcal {N}}_{1})={1 \\over 2}\\left\\{\\operatorname {tr} \\left({\\boldsymbol {\\Sigma }}_{1}^{-1}{\\boldsymbol {\\Sigma }}_{0}\\right)+\\left({\\boldsymbol {\\mu }}_{1}-{\\boldsymbol {\\mu }}_{0}\\right)^{\\rm {T}}{\\boldsymbol {\\Sigma }}_{1}^{-1}({\\boldsymbol {\\mu }}_{1}-{\\boldsymbol {\\mu }}_{0})-k+\\ln {|{\\boldsymbol {\\Sigma }}_{1}| \\over |{\\boldsymbol {\\Sigma }}_{0}|}\\right\\},}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Sigma_0 = \\Sigma_z \\\\ \n",
    "\\mu_0 = \\mu_z, \\\\ \n",
    "\\Sigma_1 = I, \\\\ \n",
    "\\mu_1 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$KL(q_{\\theta_j}(z_j, S_j) || p(z_j))= \\frac 1 2 ( \\sum_i \\sigma_i + \\mu_z^T\\mu_z - 2D -\\sum_i\\ln{\\sigma_i}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPrior():\n",
    "    def __init__(self, N_batch, in_x, in_z, dim_middle, dim_out, n_layers):\n",
    "        \n",
    "        super(self.__class__, self).__init__()\n",
    "    \n",
    "        self.dim_z = in_z\n",
    "        \n",
    "        self.encoder = Encoder(N_batch, in_x, dim_middle, dim_out, n_layers)\n",
    "        self.decoder = Decoder(in_z, in_x, dim_middle, dim_out, n_layers)\n",
    "        \n",
    "        self.ELBO = []\n",
    " \n",
    "    def forward(self, X, y):\n",
    "        \"\"\"\n",
    "        takes dataset S and produces ELBO lower bound for p(D) where D - dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # now let compute stohastic part\n",
    "        S = torch.cat([X,y], dim = -1)\n",
    "        mu_z, sigma_z = self.encoder(S)\n",
    "        sigma_z = sigma_z.pow(1/2)\n",
    "   \n",
    "        # firstly let compute not stochastic component that corresponds to KL term\n",
    "        KL_part = -1/2 * (sigma_z.sum()+(mu_z*mu_z).sum() - 2*self.dim_z - sigma_z.log().sum())\n",
    "        \n",
    "        #sampling\n",
    "        N_batch = S.shape[0]\n",
    "        z = torch.normal(mean = torch.zeros((N_batch, self.dim_z)), \n",
    "                          std = torch.ones((N_batch,  self.dim_z)))\n",
    "        \n",
    "        mu_y, sigma_y = self.decoder(X, z)\n",
    "        \n",
    "        log_likelihood = (-1/2*(y - mu_y)*(y-mu_y)* sigma_y.pow(2)).sum()-sigma_y.log().sum()\n",
    "        \n",
    "        loss = log_likelihood+KL_part\n",
    "        \n",
    "        self.ELBO.append(loss.cpu().data.numpy())\n",
    "        \n",
    "        return -loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeepPrior' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-3e7a5260b8d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepPrior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_z\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_middle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeepPrior' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "# оптимайзер\n",
    "from torch import optim\n",
    "model = DeepPrior(N_batch=4, in_x=1, in_z=1, dim_middle=128, dim_out=1, n_layers=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=20, lr=1e-3, verbose=True):\n",
    "    \n",
    "    num_epochs = num_epochs # total amount of full passes over training data\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train(True) # enable dropout / batch_norm training behavior\n",
    "        for (X_batch, y_batch) in train_loader:\n",
    "            \n",
    "            loss = model(torch.Tensor(X_batch), torch.Tensor(y_batch))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "          # Visualize\n",
    "        # Then we print the results for this epoch:\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time)) \n",
    "        print('current elbo: {}'.format(model.ELBO[-1]))\n",
    "        \n",
    "#         print('acc hard', accuracy((model.hard_predict(X_batch)>0.5).data.numpy(), y_batch.data.numpy() ))\n",
    "#         print('acc soft', accuracy((model.soft_predict(X_batch)>0.5).data.numpy(), y_batch.data.numpy() ))\n",
    "\n",
    "        if verbose:\n",
    "            plt.figure(figsize=(16, 6))\n",
    "            plt.subplot(221)\n",
    "            plt.title(\"ELBO\")\n",
    "            plt.xlabel(\"#iteration\")\n",
    "            plt.ylabel(\"elbo\")\n",
    "            plt.plot(model.ELBO, 'b', label = 'train_elbo')\n",
    "            plt.show()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
