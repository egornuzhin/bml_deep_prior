{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.utils.data\n",
    "import time \n",
    "from IPython import  display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w = h_\\alpha(z)$\n",
    "\n",
    "$p(w|\\alpha) = \\int_z p(z)p(w|z, \\alpha) dz = \\int_z p(z)\\delta_{h_\\alpha(z)-w_0} dz$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generator\n",
    "\n",
    "import torch.distributions as dist\n",
    "\n",
    "class ShiftedExponential(dist.Exponential):\n",
    "    def __init__(self, rate,shift, validate_args=None):\n",
    "        self.shift = shift\n",
    "        super(ShiftedExponential, self).__init__(rate, validate_args=None)\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        s = super(ShiftedExponential, self).rsample(sample_shape)\n",
    "        return s+self.shift\n",
    "\n",
    "def get_dist(mean,var):\n",
    "    l = dist.Laplace(mean,(var/2)**(1/2))\n",
    "    u = dist.Uniform(mean-(3*var)**(1/2) ,mean+(3*var)**0.5)\n",
    "    n = dist.Normal(mean,var**(1/2))\n",
    "    e = ShiftedExponential((1/var)**(1/2),mean-var**(1/2))\n",
    "    dists = [l,u,n,e]\n",
    "    d_type = int(torch.randint(4,torch.Size([1])))\n",
    "    return dists[d_type],d_type\n",
    "\n",
    "def create_samples(num_samples,mean,var,cls = 0):\n",
    "    l = dist.Laplace(mean,(var/2)**(1/2))\n",
    "    u = dist.Uniform(mean-(3*var)**(1/2) ,mean+(3*var)**0.5)\n",
    "    n = dist.Normal(mean,var**(1/2))\n",
    "    e = ShiftedExponential((1/var)**(1/2),mean-var**(1/2))\n",
    "    dists = [l,u,n,e]\n",
    "    return dists[cls].sample(torch.Size([num_samples]))\n",
    "\n",
    "def create_random_samples(num_dsets,num_samples):\n",
    "    mean_dist = dist.Uniform(-0.1 ,0.1)\n",
    "    var_dist = dist.Uniform(0.5 ,0.6)\n",
    "    means = mean_dist.sample(torch.Size([num_dsets]))\n",
    "    vars = var_dist.sample(torch.Size([num_dsets]))\n",
    "    dsets = []\n",
    "    dtypes = []\n",
    "    for mean,var in zip(means,vars):\n",
    "        current_dist, d_type = get_dist(mean,var)\n",
    "        dset = current_dist.sample(torch.Size([num_samples]))\n",
    "        dsets += dset,\n",
    "        dtypes += d_type,\n",
    "    return torch.stack(dsets), torch.tensor(dtypes)\n",
    "\n",
    "num_dsets = 10000\n",
    "num_samples = 200\n",
    "train_size = num_dsets*num_samples\n",
    "X,y = create_random_samples(num_dsets,num_samples)\n",
    "X = X.unsqueeze(-1)\n",
    "bs = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\\\n",
    "            torch.utils.data.TensorDataset(\\\n",
    "            *(torch.Tensor(X),y)),\\\n",
    "            batch_size=bs,shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_block(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        \n",
    "        super(fc_block, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(dim_in, dim_out), \n",
    "                                nn.LayerNorm(dim_out),\n",
    "                                nn.ReLU())\n",
    "                                \n",
    "    def forward(self, C):\n",
    "\n",
    "        return self.fc(C) \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_z, in_x, dim_middle, dim_out, n_layers):\n",
    "        \"\"\" \n",
    "        nn that maps from latent space Z and the sample X to the target variable Y\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "#         in_z = 0\n",
    "        self.fc0 = nn.Linear(in_z+in_x, dim_middle) \n",
    "        self.fc1 = fc_block(dim_middle, dim_middle)\n",
    "                \n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            fc_block(dim_middle, dim_middle)\n",
    "            for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_last_mu = nn.Linear(dim_middle, dim_out)\n",
    "        self.fc_last_sigma = nn.Linear(dim_middle, dim_out)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    " \n",
    "    def forward(self, X, z):\n",
    "        \"\"\"\n",
    "        takes x and latent z and produces target y\n",
    "        \"\"\"\n",
    "  \n",
    "        \n",
    "        c = torch.cat([X, z], dim=-1)\n",
    "#         c = X\n",
    "        if verbose: print('c', c.shape)\n",
    "        c = self.relu(self.fc0(c))\n",
    "    \n",
    "        c, c_prev = self.fc1(c), c\n",
    "        if verbose: print('c', c.shape)\n",
    "\n",
    "        for fc in self.layer_stack:\n",
    "            c, c_prev = fc(c+c_prev), c\n",
    "\n",
    "        if verbose: print('c', c.shape)\n",
    "            \n",
    "        self.mu_y = self.fc_last_mu(c)\n",
    "        self.sigma_y = self.sigmoid(self.fc_last_sigma(c))*0.1+0.001\n",
    "\n",
    "\n",
    "        return self.mu_y, self.sigma_y\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_in, dim_middle, dim_out, n_layers):\n",
    "        \"\"\" \n",
    "        nn that maps dataset to the latent z that represent it      \n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # пока самая простая \n",
    "        self.fc1 = nn.Linear(dim_in, dim_middle)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            fc_block(dim_middle, dim_middle)\n",
    "            for _ in range(n_layers)])\n",
    "            \n",
    "        self.fc_last_mu = nn.Linear(dim_middle, dim_out)\n",
    "        self.fc_last_sigma = nn.Linear(dim_middle, dim_out)\n",
    "        \n",
    " \n",
    "    def forward(self, S):\n",
    "        \"\"\"\n",
    "        takes dataset S and produces mu and sigma of normal distribution z\n",
    "        \"\"\"\n",
    "\n",
    "        S = self.relu(self.fc1(S))  \n",
    "        S_prev = S\n",
    "        \n",
    "        for fc in self.layer_stack:\n",
    "            S, S_prev = fc(S+S_prev), S\n",
    "            \n",
    "        if verbose: print('S', S.shape)\n",
    "        S = S.mean(0)\n",
    "        \n",
    "        self.mu_z = self.fc_last_mu(S)\n",
    "        self.s = self.fc_last_sigma(S)\n",
    "        self.sigma_z = self.s.exp() # экспонента чтобы не было отрицательных значений дисперсии\n",
    "\n",
    "        return self.mu_z, self.sigma_z, self.s\n",
    "    \n",
    "class DeepPrior(nn.Module):\n",
    "    def __init__(self, in_x, in_z, dim_middle, dim_out, n_enc_layers, n_dec_layers):\n",
    "        \n",
    "        super(DeepPrior, self).__init__()\n",
    "    \n",
    "        self.dim_z = in_z\n",
    "        \n",
    "        in_y = 0\n",
    "        self.in_z = in_z\n",
    "        \n",
    "        self.encoder = Encoder(in_x+in_y, self.in_z, self.in_z, n_enc_layers)\n",
    "        self.decoder = Decoder(in_z, in_x, dim_middle, dim_out, n_dec_layers)\n",
    "        \n",
    "        self.ELBO = {}\n",
    " \n",
    "    def forward(self, y, X):\n",
    "        \"\"\"\n",
    "        takes dataset S and produces ELBO lower bound for p(D) where D - dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # now let compute stohastic part\n",
    "        S = X\n",
    "        if verbose: print('S', S.shape)\n",
    "        mu_z, sigma_z, _  = self.encoder(S)\n",
    "        if verbose: print('mu_z, sigma_z', mu_z.shape, sigma_z.shape)\n",
    "        sigma_z = sigma_z.pow(1/2)\n",
    "   \n",
    "        # firstly let compute not stochastic component that corresponds to KL term\n",
    "        KL_part = -1/2 * (sigma_z.sum()+(mu_z*mu_z).sum() - 2*self.dim_z - sigma_z.log().sum())\n",
    "        \n",
    "        if verbose: print('KL_part', KL_part)\n",
    "            \n",
    "        #sampling\n",
    "        N_batch = S.shape[0]\n",
    "        z = torch.normal(mean = torch.zeros((1, self.in_z)), \n",
    "                          std = torch.ones((1,  self.in_z)))\n",
    "        \n",
    "        z = z.repeat(N_batch, 1)\n",
    "        \n",
    "        if verbose: print('z', z.shape)\n",
    "            \n",
    "        z = mu_z+sigma_z*z\n",
    "        if verbose: print('z', z.shape)\n",
    "        mu_y, sigma_y = self.decoder(X, z)\n",
    "        \n",
    "        if verbose: print('mu_y, sigma_y', mu_y.shape, sigma_y.shape)\n",
    "        \n",
    "        \n",
    "        log_likelihood = -1/2*(y - mu_y)*(y-mu_y)/ sigma_y.pow(2)\n",
    "        if verbose: print('log_likelihood', log_likelihood.shape) \n",
    "            \n",
    "        log_likelihood = log_likelihood.sum()-sigma_y.log().sum()\n",
    "        if verbose: print('log_likelihood', log_likelihood.shape)\n",
    "\n",
    "        ratio = float(np.max([l/N_batch, 1]))\n",
    "        loss = ratio*log_likelihood+KL_part\n",
    "        \n",
    "        l_np = float(loss.cpu().data.numpy())\n",
    "\n",
    "        try:\n",
    "            self.ELBO[int(y.data.numpy().flatten())].append(l_np)\n",
    "        except KeyError:\n",
    "            self.ELBO[int(y.data.numpy().flatten())]=[l_np]\n",
    "\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def predict(self, X, y, X_pred):\n",
    "        \"\"\"\n",
    "        S - dataset (tuple with two tensors with shape (Points, 1))\n",
    "        X - points to predict y in (tensor with shape (N_points, ))\n",
    "        \"\"\"\n",
    "        \n",
    "        S = torch.cat([X, y], dim = -1)\n",
    "        if verbose: print('S', S.shape)\n",
    "        mu_z, sigma_z, _  = self.encoder(S)\n",
    "        \n",
    "#         sigma_z = sigma_z.pow(1/2)\n",
    "#         N_batch = 1\n",
    "#         z = torch.normal(mean = torch.zeros((N_batch, self.in_z)), \n",
    "#                           std = torch.ones((N_batch,  self.in_z)))      \n",
    "#         if verbose: print('z', z.shape)\n",
    "            \n",
    "        z = mu_z.unsqueeze(0) #+sigma_z*z\n",
    "        if verbose: print('z', z.shape)\n",
    "            \n",
    "        z = z.repeat(X_pred.shape[0], 1)\n",
    "        if verbose: \n",
    "            print('z', z.shape)\n",
    "            print('X', X.shape)\n",
    "            \n",
    "        mu_y, sigma_y = self.decoder(X_pred, z)\n",
    "        \n",
    "        return mu_y, sigma_y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs=20, batchsize = 10, verbose=True, plot_every = 100):\n",
    "    \n",
    "    start_time = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train(True) # enable dropout / batch_norm training behavior\n",
    "        for (X_batch, y_batch) in train_loader:\n",
    "            \n",
    "            print('X_batch', X_batch.shape)\n",
    "            print('y_batch', y_batch.shape)\n",
    "            \n",
    "            loss = model(y_batch, X_batch[0])\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "        # Visualize\n",
    "        # Then we print the results for this epoch:\n",
    "        if verbose: # and epoch % plot_every == 0\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time)) \n",
    "            start_time = time.time()\n",
    "\n",
    "            ELBO = np.array(list(model.ELBO.values()))\n",
    "            print('current elbo: {}'.format(ELBO[0][-1]))\n",
    "            \n",
    "\n",
    "            plt.figure(figsize=(16, 6))\n",
    "            plt.subplot(221)\n",
    "            plt.title(\"ELBO\")\n",
    "            plt.xlabel(\"#iteration\")\n",
    "            plt.ylabel(\"elbo\")\n",
    "            plt.plot(ELBO.T, label = 'train_elbo')\n",
    "            plt.show()\n",
    "           \n",
    "    return X_0, y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимайзер\n",
    "from torch import optim\n",
    "from itertools import chain\n",
    "model = DeepPrior(in_x=1, in_z=128, dim_middle=128, dim_out=1, n_enc_layers=4, n_dec_layers=12)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch torch.Size([1, 200, 1])\n",
      "y_batch torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #3 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-77e753d80951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-ff46e03a0dc9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, batchsize, verbose, plot_every)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-1815292ecfed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y, X)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0msigma_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_likelihood'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #3 'other'"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "X_0, y_0 = train_model(model, num_epochs=10000, batchsize = 50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
